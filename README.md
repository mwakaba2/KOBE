# KOBE

### [Project](https://sites.google.com/view/kobe2019) | [arXiv](https://arxiv.org/abs/1903.12457)

Towards **K**n**O**wledge-**B**ased p**E**rsonalized Product Description Generation in E-commerce.<br>
[Qibin Chen](https://www.qibin.ink)<sup>\*</sup>, [Junyang Lin](https://justinlin610.github.io)<sup>\*</sup>, Yichang Zhang, [Hongxia Yang](https://sites.google.com/site/hystatistics/home), [Jingren Zhou](http://www.cs.columbia.edu/~jrzhou/), [Jie Tang](http://keg.cs.tsinghua.edu.cn/jietang/).<br>
<sup>*</sup>Equal contribution.<br>
In KDD 2019 (Applied Data Science Track)

## Prerequisites

- Linux or macOS
- Python 3
- PyTorch >= 1.0.1
- NVIDIA GPU + CUDA cuDNN

## Getting Started

### Installation

Clone this repo.

```bash
git clone https://github.com/THUDM/KOBE
cd KOBE
```

Please install dependencies by

```bash
pip install -r requirements.txt
```

### Dataset


### Training


#### Start training

- Different configurations for models in the paper are stored under the `configs/` directory. Launch a specific experiment with `--config` to specify the path to your desired model config and `--expname` to specify the name/number of this experiment which will be used in logging.
- We include three config files here: the baseline, KOBE without adding external knowledge, and full KOBE model.

- Baseline

```bash
python core/train.py --config configs/baseline.yaml --expname baseline
```

- KOBE without adding knowledge

```bash
python core/train.py --config configs/aspect_user.yaml --expname aspect-user
```

- KOBE

```bash
python core/train.py --config configs/aspect_user_knowledge.yaml --expname aspect-user-knowledge
```

The default `batch size` is set to 64.
If you are having OOM problems, try to decrease it with the flag `--batch-size`.

#### Track training progress

- You can use TensorBoard. It can take (roughly) 12 hours for the training to stop. To get comparable results in paper, you need to train for even longer (by editing `epoch` in the config files). However, the current setting is enough to demonstrate the effectiveness of our model.

```bash
tensorboard --logdir experiments --port 6006
```

### Generation

- During training, the generated descriptions on the test set is saved at `experiments/<expname>/candidate.txt` and the ground truth is at `reference.txt`. This is generated by greedy search to save time in training and doesn't block repetitive terms.
- To do beam search with `beam width = 10`, run the following command.


### Evaluation

- BLEU
- DIVERSITY

#### Generating predictions 

Baseline model - product title and description 
```bash
python3.5 core/api.py --config /home/jupyter/KOBE/configs/baseline.yaml --test-src-file /home/jupyter/data/baseline/evaluation.src  --prediction-file /home/jupyter/data/baseline/eval_prediction.txt  --pretrain /home/jupyter/KOBE/experiments/baseline/best_bleu_checkpoint.pt --mode eval --batch-size 32 --beam-size 10 --use-cuda
```

## Cite

Please cite our paper if you use this code in your own work:

```
@article{chen2019towards,
  title={Towards Knowledge-Based Personalized Product Description Generation in E-commerce},
  author={Chen, Qibin and Lin, Junyang and Zhang, Yichang and Yang, Hongxia and Zhou, Jingren and Tang, Jie},
  journal={arXiv preprint arXiv:1903.12457},
  year={2019}
}
```

